---
type: assignment
date: 2024-10-25
tag: 'Assignment #4'
title: 'Comparing models and mitigating bias'
pdf: https://storage.googleapis.com/cmu-llms/2024/hw4/hw4.pdf
starter_code: https://storage.googleapis.com/cmu-llms/2024/hw4/hw4-starter-code-2024.1.1.zip
submission_template: /static_files/assignments/coming_soon.zip
hide_from_announcments: true
due_event: 
    type: due
    date: 2024-11-05T14:00:00+3:30
    description: 'Assignment #4 due'
---

In this homework, you will improve your understanding of how different models can behave very differently given the same prompt.
You will also measure the amount of toxic text generated by different models.
You will then experiment with solving a classification task using few-shot prompting techniques, and you will implement a method to mitigate label bias.
