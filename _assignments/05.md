---
type: assignment
date: 2024-11-05
tag: 'Assignment #5'
title: 'Improving training efficiency'
pdf: https://storage.googleapis.com/cmu-llms/2024/hw5/11_667_2024_homeworks-2.pdf
starter_code: https://storage.googleapis.com/cmu-llms/2024/hw5/hw5-starter.zip
submission_template: https://storage.googleapis.com/cmu-llms/2024/hw5/homework5_submission_template.tex
hide_from_announcments: true
due_event: 
    type: due
    date: 2024-11-14T14:00:00+3:30
    description: 'Assignment #5 due'
---

In this homework, you will explore strategies for making language models more efficient both in their training and inference phases. Large language models require significant computational resources due to their high memory consumption, large parameter counts, and the extensive compute needed for both training and serving. Efficiently training these models involves optimizing memory usage and using distributed training setups, which allow us to leverage multiple GPUs to process larger models or batches without running out of memory.
