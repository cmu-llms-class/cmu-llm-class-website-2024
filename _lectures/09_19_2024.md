---
type: lecture
date: 2024-09-19
title: Alignment, RLHF, jail-breaking
module: Making foundation language models useful
hide_from_announcments: true
links: 
 - url: coming soon
   name: slides
---
Readings:
 - [RLHF with direct preference optimization](https://arxiv.org/pdf/2203.02155)
 - [DPO vs PPO](https://arxiv.org/pdf/2404.10719v1)
 - [Are aligned models adversarially aligned?](https://proceedings.neurips.cc/paper_files/paper/2023/file/c1f0b856a35986348ab3414177266f75-Paper-Conference.pdf)

